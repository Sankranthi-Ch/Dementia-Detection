{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep CNN",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoeOhZhQfxVo",
        "outputId": "e266a925-7f21-42af-eb34-f6584dd0e3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.2)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nilearn) (4.2.6)\n",
            "Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->nilearn) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->nilearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "get_ipython().system(u'pip install nilearn')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import linalg\n",
        "\n",
        "from nilearn import datasets\n",
        "from nilearn.input_data import NiftiMasker\n",
        "\n",
        "from nilearn.image import smooth_img\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import keras\n",
        "\n",
        "n_subjects = 416\n",
        "\n",
        "oasis_dataset = datasets.fetch_oasis_vbm(n_subjects=n_subjects)\n",
        "gray_matter_map_filenames = oasis_dataset.gray_matter_maps\n",
        "gm_imgs = gray_matter_map_filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCEOkUtQgIRc",
        "outputId": "26499a2f-650f-41b5-f400-9462f99aed42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nilearn/datasets/struct.py:660: UserWarning: Only 403 subjects are available in the DARTEL-normalized version of the dataset. All of them will be used instead of the wanted 416\n",
            "  % n_subjects)\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/datasets/struct.py:774: UserWarning: `legacy_format` will default to `False` in release 0.11. Dataset fetchers will then return pandas dataframes by default instead of recarrays.\n",
            "  warnings.warn(_LEGACY_FORMAT_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cdr = oasis_dataset.ext_vars['cdr'].astype(float)\n",
        "cdr_numpy_arr = np.array(cdr)\n",
        "for i in range(len(cdr_numpy_arr)):\n",
        "    if(np.isnan(cdr_numpy_arr[i])): cdr_numpy_arr[i] = 1\n",
        "    \n",
        "    elif(cdr_numpy_arr[i] > 0.0): cdr_numpy_arr[i] = 1"
      ],
      "metadata": {
        "id": "0kD3qF9OgOb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgArr = []\n",
        "\n",
        "for imgUrl in gray_matter_map_filenames:\n",
        "    result_img = smooth_img(imgUrl, fwhm=1)\n",
        "    imgArr.append(result_img.get_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNLNrFS2gTg0",
        "outputId": "7f9d5c83-67b4-45a8-d895-b709d9796388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
            "\n",
            "* deprecated from version: 3.0\n",
            "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "x_test = []\n",
        "\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "rshapedImgArr = []\n",
        "\n",
        "for img in imgArr:\n",
        "    newImg = [cv2.resize(each_slice,(50,50)) for each_slice in img]#Reducing slice count\n",
        "    newImg = np.array(newImg)\n",
        "    rshapedImgArr.append(newImg)\n",
        "    #print(newImg)\n",
        "    \n",
        "label = cdr_numpy_arr"
      ],
      "metadata": {
        "id": "4BK00siigXuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = keras.utils.np_utils.to_categorical(cdr_numpy_arr, 2)\n",
        "\n",
        "much_data = []\n",
        "\n",
        "for num, img in enumerate(rshapedImgArr):\n",
        "    much_data.append([img,label[num]])"
      ],
      "metadata": {
        "id": "DDhE4x9wgcFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE_PX_X = 50\n",
        "IMG_SIZE_PX_Y = 50\n",
        "SLICE_COUNT = 91\n",
        "\n",
        "n_classes = 2\n",
        "batch_size = 10\n",
        "\n",
        "x = tf.placeholder('float')\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "keep_rate = 0.8"
      ],
      "metadata": {
        "id": "0WiOP5mfgf-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3d(x, W):\n",
        "    conv = tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
        "    conv = tf.nn.dropout(conv, 0.5)\n",
        "    return conv\n",
        "\n",
        "def maxpool3d(x):\n",
        "    #                        size of window         movement of window as you slide about\n",
        "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')"
      ],
      "metadata": {
        "id": "OvDKcWh2gnbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_neural_network(x):\n",
        "    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n",
        "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
        "               #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n",
        "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
        "               #                                  64 features\n",
        "               'W_fc':tf.Variable(tf.random_normal([248768,1024])),\n",
        "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
        "\n",
        "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
        "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
        "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
        "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
        "\n",
        "    #                            image X      image Y        image Z\n",
        "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX_X, IMG_SIZE_PX_Y, SLICE_COUNT, 1])\n",
        "\n",
        "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
        "    conv1 = maxpool3d(conv1)\n",
        "\n",
        "\n",
        "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
        "    conv2 = maxpool3d(conv2)\n",
        "\n",
        "    fc = tf.reshape(conv2,[-1, 248768])\n",
        "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
        "    fc = tf.nn.dropout(fc, keep_rate)\n",
        "\n",
        "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "wF3AP2Rlgr-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_neural_network(x):\n",
        "    prediction = convolutional_neural_network(x)\n",
        "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
        "    \n",
        "    file = open(\"output.txt\", \"w\");\n",
        "    \n",
        "    hm_epochs = 1000\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        \n",
        "        successful_runs = 0\n",
        "        total_runs = 0\n",
        "        \n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            \n",
        "            train_data, validation_data = train_test_split(much_data, train_size=0.8)\n",
        "            \n",
        "            for data in train_data:\n",
        "                total_runs += 1\n",
        "                try:\n",
        "                    X = data[0]\n",
        "                    Y = data[1]\n",
        "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
        "                    epoch_loss += c\n",
        "                    successful_runs += 1\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "                    #print(str(e))\n",
        "            \n",
        "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "            file.write('Epoch'+ str(epoch+1)+ 'completed out of'+str(hm_epochs)+'loss:'+str(epoch_loss))\n",
        "\n",
        "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "\n",
        "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "            file.write('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "            \n",
        "        print('Done. Finishing accuracy:')\n",
        "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "        \n",
        "        print('fitment percent:',successful_runs/total_runs)\n",
        "        \n",
        "        file.write('Done. Finishing accuracy:')\n",
        "        file.write('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
        "        \n",
        "        file.write('fitment percent:',successful_runs/total_runs)\n",
        "\n",
        "train_neural_network(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-oTEcybg2E_",
        "outputId": "857a9665-f71b-45a8-d0bd-1ce8f275f0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed out of 1000 loss: 323704935.2421875\n"
          ]
        }
      ]
    }
  ]
}